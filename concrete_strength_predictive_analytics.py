# -*- coding: utf-8 -*-
"""concrete_strength_predictive_analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mqy_6Lp9jBx8I9GoKqNYrXXsr0rt-EP3

# Setup

## Install ucimlrepo package to load the dataset
"""

!pip install ucimlrepo

"""## Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import seaborn as sns

"""## Data Loading
- dataset link: [click here!](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)
- ucirepo has a fetch method that returns a pandas dataframe of the dataset called fetch_ucirepo(id)
- to get the dataset id
  * go to the dataset link
  * click the "import in python" button and copy the code in python notebook
  * or check the url after the word "dataset/"

- ucirepo returns the dataset at data object
- inside the data object listed the features, targets, and original
"""

from ucimlrepo import fetch_ucirepo
concrete_compressive_strength = fetch_ucirepo(id=165)

concrete_data = concrete_compressive_strength.data.original
concrete_data

"""# Data Understanding

- dataset link: [click here!](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)

## EDA - Variable Description

Variable Description
* Features : the component used for the concrete mixture
    1. Cement (component 1): Numerical value representing the amount of cement in kilograms in a cubic meter mixture.
    2. Blast Furnace Slag (component 2): Numerical value representing the amount of blast furnace slag in kilograms in a cubic meter mixture.
    3. Fly Ash (component 3): Numerical value representing the amount of fly ash in kilograms in a cubic meter mixture.
    4. Water (component 4): Numerical value representing the amount of water in kilograms in a cubic meter mixture.
    5. Superplasticizer (component 5): Numerical value representing the amount of superplasticizer in kilograms in a cubic meter mixture.
    6. Coarse Aggregate (component 6): Numerical value representing the amount of coarse aggregate in kilograms in a cubic meter mixture.
    7. Fine Aggregate (component 7): Numerical value representing the amount of fine aggregate in kilograms in a cubic meter mixture.
    8. Age: Numerical value representing the age of the concrete in days (1 to 365).

* Target:
  - Concrete Compressive Strength: Numerical value representing the compressive strength of the concrete in megapascals (MPa).

### Check dataset sample
"""

concrete_data.info()

"""### Check for missing values
> assumption: the 0's means that component wasn't used for the mixture
"""

concrete_data.describe()

"""## EDA - Check for Outliers

### Check for outliers from each feature
"""

sns.boxplot(x=concrete_data['Cement'])

sns.boxplot(x=concrete_data['Blast Furnace Slag'])

sns.boxplot(x=concrete_data['Fly Ash'])

sns.boxplot(x=concrete_data['Water'])

sns.boxplot(x=concrete_data['Superplasticizer'])

sns.boxplot(x=concrete_data['Coarse Aggregate'])

sns.boxplot(x=concrete_data['Fine Aggregate'])

sns.boxplot(x=concrete_data['Age'])

"""### Handle Outliers
- there are a few outliers that need to be cleaned
- using IQR method to clean outliers
"""

Q1 = concrete_data.quantile(0.25, numeric_only=True)
Q3 = concrete_data.quantile(0.75, numeric_only=True)

IQR = Q3 - Q1
left_Q1, right_Q1 = concrete_data.align((Q1 - 1.5 * IQR), axis=1, copy=False)
left_Q3, right_Q3 = concrete_data.align((Q3 + 1.5 * IQR), axis=1, copy=False)
concrete_data = concrete_data[~((left_Q1 < right_Q1) | (left_Q3 > right_Q3)).any(axis=1)]

concrete_data.shape

"""## EDA - Univariate Analysis"""

concrete_data.hist(bins=50, figsize=(20,15))
plt.show()

"""## EDA - Multivariate Analysis
- analize the correlation between the concrete compressive strength with other features
"""

sns.pairplot(concrete_data, diag_kind='kde')

"""### Evaluate Correlation Score"""

plt.figure(figsize=(10,8))
correlation_matrix = concrete_data.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidth=0.5)
plt.title("Correlation Matrix for Numerical Features")

"""# Data Preparation
- split features and target
- split into train and test
- standardization

## Split Features and Target
"""

X = concrete_data.drop(['Concrete compressive strength'], axis=1)
y = concrete_data['Concrete compressive strength']

"""## Split into train and test"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)

print(f"Total # of samples in dataset: {len(X)}")
print(f"Total # of samples in train dataset: {len(X_train)}")
print(f"Total # of samples in test dataset: {len(X_test)}")

"""## Standardization

- when using StandardScaler the dataframe will change into numpy.ndarray
- to prevent this simply store the index and columns
- and then create a dataframe using those index and columns combined with the scaled data
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

index, columns = X_train.index, X_train.columns

X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train, index=index, columns=columns)
X_train.head()

X_train.describe().round(4)

"""# Modelling

Algorithm used :
- K-Nearest Neighbor
- Support Vector Regression
- Random Forest
- Boosting

## Root Mean Squared Error function
"""

from sklearn.metrics import mean_squared_error

def rmse(y_pred, y_true):
  return np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))

"""## K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=6)
knn.fit(X_train, y_train)

result = rmse(y_pred=knn.predict(X_train), y_true=y_train)
result

"""## Support Vector Regression"""

from sklearn.svm import SVR

svr = SVR(kernel='rbf')
svr.fit(X_train, y_train)

result = rmse(y_pred=svr.predict(X_train), y_true=y_train)
result

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=16, max_depth=8, random_state=69, n_jobs=-1)
rf.fit(X_train, y_train)

result = rmse(y_pred=rf.predict(X_train), y_true=y_train)
result

"""## XGBoost | Extreme Gradient Boosting"""

from xgboost import XGBRegressor

xgbr = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=16,
    max_depth=8,
    learning_rate=0.3,
    subsample=0.5,
    colsample_bytree=0.5
)

xgbr.fit(X_train, y_train)

result = rmse(y_pred=xgbr.predict(X_train), y_true=y_train)
result

"""# Model Evaluation

### scale the X_test using standardscaler
"""

t_index, t_columns = X_test.index, X_test.columns

X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, index=t_index, columns=t_columns)
X_test.head()

"""### define a dataframe to evaluate model"""

model_result = pd.DataFrame(columns=['train_rmse', 'test_rmse'], index=['KNN', 'SVR', 'RF', 'XGR'])

"""### iterate over each model to get rmse of train and test"""

model_dict = {
    'KNN': knn,
    'SVR': svr,
    'RF': rf,
    'XGR': xgbr
}

for name, model in model_dict.items():
  model_result.loc[name, 'train_rmse'] = rmse(y_pred=model.predict(X_train), y_true=y_train)
  model_result.loc[name, 'test_rmse'] = rmse(y_pred=model.predict(X_test), y_true=y_test)

model_result

"""### plot the rmse"""

fig, ax = plt.subplots()
model_result.sort_values(by='test_rmse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""### Try predicting using all model"""

prediction = X_test.iloc[:10].copy()
pred_dict = {'y_true': y_test[:10]}

for name, model in model_dict.items():
  pred_dict[name+'_prediction'] = model.predict(prediction).round(1)

pd.DataFrame(pred_dict)